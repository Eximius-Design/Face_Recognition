{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from scipy import misc\n",
    "from Detection import Detection\n",
    "from Recognition import Recognition\n",
    "import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path for reading the video\n",
    "Input_video_path = \"../Dataset/videos/vid1.mp4\"\n",
    "Output_video_path = \"../Outputs/Detection_Recognition_outputs/\"\n",
    "Output_video_name = \"Detection_Recognition.avi\"\n",
    "classifier_pickle_file = \"../util/classifier.pkl\"\n",
    "O_path = Output_video_path + Output_video_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path for the detection and recognition pb files\n",
    "Detection_model_path = \"../Models/Detection_mtcnn.pb\"\n",
    "Recognition_model_path = \"../Models/Recognition_facenet.pb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection Model Graph Initialized\n",
      "Recognition Model Graph Initialized\n"
     ]
    }
   ],
   "source": [
    "#Instances of detection and recognition are being created.\n",
    "#Instances are created to avoid loading the graphs and sessions again and again for every frame.\n",
    "detection = Detection(Detection_model_path)\n",
    "recognition = Recognition(Recognition_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(classifier_pickle_file, 'rb') as infile:\n",
    "    model,class_names = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] percentage of bounding box in total image : 2.66\n",
      "[0.98241956]\n",
      "[INFO] percentage of bounding box in total image : 0.12\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 1\n",
      "[INFO] Processing Speed: 0.38083525991983974  FPS\n",
      "[INFO] Time Per Frame: 2.625807285308838\n",
      "[INFO] percentage of bounding box in total image : 2.82\n",
      "[0.96354355]\n",
      "[INFO] percentage of bounding box in total image : 0.13\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 2\n",
      "[INFO] Processing Speed: 11.765921695247126  FPS\n",
      "[INFO] Time Per Frame: 0.0849912166595459\n",
      "[INFO] percentage of bounding box in total image : 2.89\n",
      "[0.97803214]\n",
      "[INFO] percentage of bounding box in total image : 0.13\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 3\n",
      "[INFO] Processing Speed: 12.438291979158326  FPS\n",
      "[INFO] Time Per Frame: 0.08039689064025879\n",
      "[INFO] percentage of bounding box in total image : 2.76\n",
      "[0.98691626]\n",
      "[INFO] total boxes: 1\n",
      "[INFO] Processing Frame: 4\n",
      "[INFO] Processing Speed: 14.18758456459382  FPS\n",
      "[INFO] Time Per Frame: 0.07048416137695312\n",
      "[INFO] percentage of bounding box in total image : 2.91\n",
      "[0.97763557]\n",
      "[INFO] total boxes: 1\n",
      "[INFO] Processing Frame: 5\n",
      "[INFO] Processing Speed: 13.78934148666864  FPS\n",
      "[INFO] Time Per Frame: 0.07251977920532227\n",
      "[INFO] percentage of bounding box in total image : 3.00\n",
      "[0.973146]\n",
      "[INFO] percentage of bounding box in total image : 0.12\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 6\n",
      "[INFO] Processing Speed: 13.374310048499884  FPS\n",
      "[INFO] Time Per Frame: 0.07477021217346191\n",
      "[INFO] percentage of bounding box in total image : 2.80\n",
      "[0.98339915]\n",
      "[INFO] percentage of bounding box in total image : 0.14\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 7\n",
      "[INFO] Processing Speed: 13.209615802518904  FPS\n",
      "[INFO] Time Per Frame: 0.07570242881774902\n",
      "[INFO] percentage of bounding box in total image : 2.74\n",
      "[0.97586977]\n",
      "[INFO] percentage of bounding box in total image : 0.11\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 8\n",
      "[INFO] Processing Speed: 13.414604035603373  FPS\n",
      "[INFO] Time Per Frame: 0.07454562187194824\n",
      "[INFO] percentage of bounding box in total image : 2.96\n",
      "[0.98367118]\n",
      "[INFO] total boxes: 1\n",
      "[INFO] Processing Frame: 9\n",
      "[INFO] Processing Speed: 13.962443283765925  FPS\n",
      "[INFO] Time Per Frame: 0.07162070274353027\n",
      "[INFO] percentage of bounding box in total image : 2.83\n",
      "[0.9733844]\n",
      "[INFO] percentage of bounding box in total image : 0.13\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 10\n",
      "[INFO] Processing Speed: 13.32697006574035  FPS\n",
      "[INFO] Time Per Frame: 0.07503581047058105\n",
      "[INFO] percentage of bounding box in total image : 2.86\n",
      "[0.98564813]\n",
      "[INFO] percentage of bounding box in total image : 0.13\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 11\n",
      "[INFO] Processing Speed: 13.137745257724209  FPS\n",
      "[INFO] Time Per Frame: 0.07611656188964844\n",
      "[INFO] percentage of bounding box in total image : 0.15\n",
      "[INFO] total boxes: 1\n",
      "[INFO] Processing Frame: 12\n",
      "[INFO] Processing Speed: 18.258729294995973  FPS\n",
      "[INFO] Time Per Frame: 0.05476832389831543\n",
      "[INFO] percentage of bounding box in total image : 2.93\n",
      "[0.97823077]\n",
      "[INFO] percentage of bounding box in total image : 0.13\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 13\n",
      "[INFO] Processing Speed: 14.716081609739838  FPS\n",
      "[INFO] Time Per Frame: 0.06795287132263184\n",
      "[INFO] percentage of bounding box in total image : 2.85\n",
      "[0.98227203]\n",
      "[INFO] percentage of bounding box in total image : 0.14\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 14\n",
      "[INFO] Processing Speed: 12.657999251560256  FPS\n",
      "[INFO] Time Per Frame: 0.07900142669677734\n",
      "[INFO] percentage of bounding box in total image : 2.96\n",
      "[0.96978567]\n",
      "[INFO] percentage of bounding box in total image : 0.14\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 15\n",
      "[INFO] Processing Speed: 13.253778506672228  FPS\n",
      "[INFO] Time Per Frame: 0.07545018196105957\n",
      "[INFO] percentage of bounding box in total image : 2.94\n",
      "[0.97453043]\n",
      "[INFO] percentage of bounding box in total image : 0.12\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 16\n",
      "[INFO] Processing Speed: 13.260818291900232  FPS\n",
      "[INFO] Time Per Frame: 0.07541012763977051\n",
      "[INFO] percentage of bounding box in total image : 3.01\n",
      "[0.98281465]\n",
      "[INFO] percentage of bounding box in total image : 0.14\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 17\n",
      "[INFO] Processing Speed: 14.036410366245448  FPS\n",
      "[INFO] Time Per Frame: 0.0712432861328125\n",
      "[INFO] percentage of bounding box in total image : 3.16\n",
      "[0.97931147]\n",
      "[INFO] percentage of bounding box in total image : 0.15\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 18\n",
      "[INFO] Processing Speed: 14.029180185302874  FPS\n",
      "[INFO] Time Per Frame: 0.07128000259399414\n",
      "[INFO] percentage of bounding box in total image : 3.07\n",
      "[0.98635407]\n",
      "[INFO] percentage of bounding box in total image : 0.15\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 19\n",
      "[INFO] Processing Speed: 14.107605580744547  FPS\n",
      "[INFO] Time Per Frame: 0.07088375091552734\n",
      "[INFO] percentage of bounding box in total image : 3.28\n",
      "[0.96196823]\n",
      "[INFO] percentage of bounding box in total image : 0.14\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 20\n",
      "[INFO] Processing Speed: 13.975283467111817  FPS\n",
      "[INFO] Time Per Frame: 0.07155489921569824\n",
      "[INFO] percentage of bounding box in total image : 3.18\n",
      "[0.97135193]\n",
      "[INFO] percentage of bounding box in total image : 0.15\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 21\n",
      "[INFO] Processing Speed: 14.044118237949185  FPS\n",
      "[INFO] Time Per Frame: 0.07120418548583984\n",
      "[INFO] percentage of bounding box in total image : 3.14\n",
      "[0.957976]\n",
      "[INFO] percentage of bounding box in total image : 0.14\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 22\n",
      "[INFO] Processing Speed: 13.853194525180996  FPS\n",
      "[INFO] Time Per Frame: 0.07218551635742188\n",
      "[INFO] percentage of bounding box in total image : 3.17\n",
      "[0.96837096]\n",
      "[INFO] percentage of bounding box in total image : 0.14\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 23\n",
      "[INFO] Processing Speed: 14.142091758462218  FPS\n",
      "[INFO] Time Per Frame: 0.07071089744567871\n",
      "[INFO] percentage of bounding box in total image : 3.11\n",
      "[0.9668813]\n",
      "[INFO] percentage of bounding box in total image : 0.13\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 24\n",
      "[INFO] Processing Speed: 13.095209698619701  FPS\n",
      "[INFO] Time Per Frame: 0.07636380195617676\n",
      "[INFO] percentage of bounding box in total image : 3.13\n",
      "[0.96652341]\n",
      "[INFO] percentage of bounding box in total image : 0.13\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 25\n",
      "[INFO] Processing Speed: 14.215088456585102  FPS\n",
      "[INFO] Time Per Frame: 0.07034778594970703\n",
      "[INFO] percentage of bounding box in total image : 3.19\n",
      "[0.95656573]\n",
      "[INFO] percentage of bounding box in total image : 0.14\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 26\n",
      "[INFO] Processing Speed: 14.718198853228715  FPS\n",
      "[INFO] Time Per Frame: 0.06794309616088867\n",
      "[INFO] percentage of bounding box in total image : 3.20\n",
      "[0.9736841]\n",
      "[INFO] percentage of bounding box in total image : 0.14\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 27\n",
      "[INFO] Processing Speed: 14.041062276335127  FPS\n",
      "[INFO] Time Per Frame: 0.07121968269348145\n",
      "[INFO] percentage of bounding box in total image : 3.24\n",
      "[0.9702165]\n",
      "[INFO] percentage of bounding box in total image : 0.14\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 28\n",
      "[INFO] Processing Speed: 14.05687359449831  FPS\n",
      "[INFO] Time Per Frame: 0.07113957405090332\n",
      "[INFO] percentage of bounding box in total image : 3.23\n",
      "[0.96746875]\n",
      "[INFO] percentage of bounding box in total image : 0.14\n",
      "[INFO] total boxes: 2\n",
      "[INFO] Processing Frame: 29\n",
      "[INFO] Processing Speed: 14.060784852731162  FPS\n",
      "[INFO] Time Per Frame: 0.07111978530883789\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d872278d9ac8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m#Inferencing the Detection model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpts\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandmarks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RT_facenet/src/Detection.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     34\u001b[0m                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_operation_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'landmarks'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                   self.graph.get_operation_by_name('box').outputs[0]]\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandmarks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandmarks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Initializing video capture from the Input_video_path.\n",
    "cap = cv2.VideoCapture(Input_video_path)\n",
    "#Variable to count frames.\n",
    "frame_count = 0\n",
    "#starting the processing time to calculate fps.\n",
    "start = time.time()\n",
    "#Ensuring the input_video_path opens without errors.\n",
    "if (cap.isOpened()== False):\n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "#getting the frame_width , frame_height from the given input video.\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "frame_area = frame_width * frame_height\n",
    "\n",
    "#creating a video file to write the output frames at output_video_path(O_path).\n",
    "\n",
    "out = cv2.VideoWriter(O_path,cv2.VideoWriter_fourcc('M','J','P','G'), 30 , (frame_width,frame_height))\n",
    "\n",
    "#Reading each and every frame in a while loop and processing/inferencing them through two models.\n",
    "grab = 0\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    frame_start_time = time.time()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_count = frame_count+1\n",
    "    if ret != True or frame_count>1000:\n",
    "        break\n",
    "    if ret == True:\n",
    "        \n",
    "        #Detection Starts :\n",
    "        \n",
    "        #Inferencing the Detection model\n",
    "        bbox, scores, landmarks = detection.detect(frame)\n",
    "        for box, pts in zip(bbox, landmarks):\n",
    "            box = box.astype('int32')\n",
    "            box_w = box[3] - box[1]\n",
    "            box_h = box[2] - box[0]\n",
    "            box_a = box_w*box_h\n",
    "            percent = box_a*100/frame_area\n",
    "            \n",
    "            # CROPPING THE FACES OUT OF THE IMAGE AND APPENDING THEM TO THE LIST\n",
    "            print('[INFO] percentage of bounding box in total image : {:.2f}'.format(percent))\n",
    "            face = np.copy(frame[box[0]:box[2] , box[1]:box[3]])\n",
    "            if percent >1.0 and face.shape[0] != 0 and face.shape[1]!= 0 and face.shape[2] !=0:\n",
    "                if grab == 0:\n",
    "                    img = face\n",
    "                grab = grab+1\n",
    "#                 plt.imshow(face)\n",
    "#                 plt.show()\n",
    "                face = preprocess.prewhiten(face)\n",
    "#                 print(face.shape)\n",
    "                embedding = recognition.recognize(face = face)\n",
    "                predictions = model.predict_proba(embedding)\n",
    "                prediction_id = model.predict(embedding)\n",
    "#                 print(\"PREDICTIONS :\",predictions)\n",
    "                best_class_indices = np.argmax(predictions, axis=1)\n",
    "#                 print(\"BEST CLASS INDICES\", best_class_indices)\n",
    "                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\n",
    "                print(best_class_probabilities)\n",
    "#                 print(\"BEST CLASS PROBABILITIES\", best_class_probabilities)\n",
    "#                 for i in range(len(best_class_indices)):\n",
    "#                     print('%4d  %s: %.3f' % (i, class_names[best_class_indices[i]], best_class_probabilities[i]))\n",
    "                frame = cv2.rectangle(frame, (box[1], box[0]), (box[3], box[2]), (255, 0, 0), 3)\n",
    "                pts = pts.astype('int32')\n",
    "                \n",
    "                class_prob = str(prediction_id[0])+\" : {:.2f}\".format(best_class_probabilities[0])\n",
    "                cv2.putText(frame, class_prob, (box[1], box[0]),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0),\n",
    "                            thickness=2, lineType=2)\n",
    "                #Uncomment this to have landmarks\n",
    "#                 for i in range(5):\n",
    "#                     frame = cv2.circle(frame, (pts[i+5], pts[i]), 4, (0, 0, 255), 8)\n",
    "                \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        #Writing to the video output\n",
    "        out.write(frame)\n",
    "        \n",
    "        frame_end_time = time.time()\n",
    "        time_per_frame = frame_end_time - frame_start_time\n",
    "        fps_frame = 1/time_per_frame\n",
    "        print('[INFO] total boxes:', len(bbox))\n",
    "        print('[INFO] Processing Frame:', frame_count)\n",
    "        print('[INFO] Processing Speed:',fps_frame,\" FPS\")\n",
    "        print('[INFO] Time Per Frame:', time_per_frame)\n",
    "        \n",
    "end = time.time()\n",
    "timet = end - start\n",
    "fps = frame_count/timet\n",
    "print(\"[INFO] NUMBER OF FRAMES:\", frame_count)\n",
    "print(\"[INFO] Detection took {:.5} seconds\".format(end - start))\n",
    "print(\"[INFO] Overall FPS: \"+ str(fps))\n",
    "\n",
    "# closing the writer and reader\n",
    "\n",
    "cap.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
